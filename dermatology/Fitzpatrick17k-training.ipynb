{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit\n",
    "Below code is taken from https://github.com/mattgroh/fitzpatrick17k/blob/main/train.py and modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import skimage\n",
    "from skimage import io\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "OUT_DIR = \"/path/outputs/fitzpatrick\"\n",
    "DATA_DIR = \"/path/dataset/fitzpatrick17k/data/finalfitz17k\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    if len(list_of_lists) == 0:\n",
    "        return list_of_lists\n",
    "    if isinstance(list_of_lists[0], list):\n",
    "        return flatten(list_of_lists[0]) + flatten(list_of_lists[1:])\n",
    "    return list_of_lists[:1] + flatten(list_of_lists[1:])\n",
    "\n",
    "\n",
    "def train_model(label, dataloaders, device, dataset_sizes, model,\n",
    "                criterion, optimizer, scheduler, num_epochs=2, eval_every=4):\n",
    "    since = time.time()\n",
    "    training_results = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                if ((epoch+1) % eval_every ) != 0:\n",
    "                    continue\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            # running_total = 0\n",
    "            print(phase)\n",
    "            # Iterate over data.\n",
    "            for batch in tqdm(dataloaders[phase]):\n",
    "                inputs = batch[\"image\"].to(device)\n",
    "                labels = batch[label]\n",
    "                labels = torch.from_numpy(np.asarray(labels)).to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    inputs = inputs.float()  # ADDED AS A FIX\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            # print(\"Loss: {}/{}\".format(running_loss, dataset_sizes[phase]))\n",
    "            print(\"Accuracy: {}/{}\".format(running_corrects,\n",
    "                                           dataset_sizes[phase]))\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            training_results.append([phase, epoch, epoch_loss, epoch_acc])\n",
    "\n",
    "            if phase == 'val':\n",
    "                print(epoch_loss, best_loss)\n",
    "                if epoch_loss < best_loss:\n",
    "                    print(\"New leading accuracy: {}\".format(epoch_acc))\n",
    "                    best_acc = epoch_acc\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                else:\n",
    "                    print(\"!!Model did not improve!!\")\n",
    "            \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    training_results = pd.DataFrame(training_results)\n",
    "    training_results.columns = [\"phase\", \"epoch\", \"loss\", \"accuracy\"]\n",
    "    return model, training_results\n",
    "\n",
    "\n",
    "class SkinDataset():\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                f\"{self.df.loc[self.df.index[idx], 'hasher']}.jpg\")\n",
    "        image = io.imread(img_name)\n",
    "        if(len(image.shape) < 3):\n",
    "            image = skimage.color.gray2rgb(image)\n",
    "\n",
    "        hasher = self.df.loc[self.df.index[idx], 'hasher']\n",
    "        high = self.df.loc[self.df.index[idx], 'high']\n",
    "        mid = self.df.loc[self.df.index[idx], 'mid']\n",
    "        low = self.df.loc[self.df.index[idx], 'low']\n",
    "        fitzpatrick = self.df.loc[self.df.index[idx], 'fitzpatrick']\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        sample = {\n",
    "                    'image': image,\n",
    "                    'high': high,\n",
    "                    'mid': mid,\n",
    "                    'low': low,\n",
    "                    'hasher': hasher,\n",
    "                    'fitzpatrick': fitzpatrick\n",
    "                }\n",
    "        return sample\n",
    "\n",
    "\n",
    "def custom_load(\n",
    "        batch_size=256,\n",
    "        num_workers=20,\n",
    "        train_dir='',\n",
    "        val_dir='',\n",
    "        image_dir=DATA_DIR):\n",
    "    val = pd.read_csv(val_dir)\n",
    "    train = pd.read_csv(train_dir)\n",
    "    class_sample_count = np.array(train[label].value_counts().sort_index())\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in train[label]])\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    sampler = WeightedRandomSampler(\n",
    "        samples_weight.type('torch.DoubleTensor'),\n",
    "        len(samples_weight),\n",
    "        replacement=True)\n",
    "    dataset_sizes = {\"train\": train.shape[0], \"val\": val.shape[0]}\n",
    "    transformed_train = SkinDataset(\n",
    "        csv_file=train_dir,\n",
    "        root_dir=image_dir,\n",
    "        transform=\n",
    "            transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.CenterCrop(size=224),  # Image net standards\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "        )\n",
    "    transformed_test = SkinDataset(\n",
    "        csv_file=val_dir,\n",
    "        root_dir=image_dir,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(size=256),\n",
    "            transforms.CenterCrop(size=224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        )\n",
    "    dataloaders = {\n",
    "        \"train\": torch.utils.data.DataLoader(\n",
    "            transformed_train,\n",
    "            batch_size=batch_size,\n",
    "            sampler=sampler,\n",
    "            # shuffle=True,\n",
    "            num_workers=num_workers),\n",
    "        \"val\": torch.utils.data.DataLoader(\n",
    "            transformed_test,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers)\n",
    "        }\n",
    "    return dataloaders, dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResNetBottom(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(ResNetBottom, self).__init__()\n",
    "        self.features = nn.Sequential(*list(original_model.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "class ResNetTop(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(ResNetTop, self).__init__()\n",
    "        self.features = nn.Sequential(*[list(original_model.children())[-1]])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.features(x)\n",
    "        x = nn.Softmax(dim=-1)(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class SqueezenetBottom(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(SqueezenetBottom, self).__init__()\n",
    "        self.features = nn.Sequential(*list(list(original_model.children())[0].children())[:15], nn.Flatten())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "class SqueezenetTop(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(SqueezenetTop, self).__init__()\n",
    "        self.features = nn.Sequential(*list(original_model.children())[1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view((-1, 512, 13, 13))\n",
    "        x = self.features(x)\n",
    "        x = x.view((-1, 1000))\n",
    "        x = nn.Softmax(dim=-1)(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def get_model_parts(model, model_name):    \n",
    "    if model_name == \"resnet\":\n",
    "        model_bottom = ResNetBottom(model)\n",
    "        model_top = ResNetTop(model)\n",
    "    elif model_name == \"squeezenet\":\n",
    "        model_bottom = SqueezenetBottom(model)\n",
    "        model_top = SqueezenetTop(model)\n",
    "    else:\n",
    "        raise ValueError(model_name)\n",
    "    return model_bottom, model_top\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "\n",
    "def initialize_model(num_classes=5, feature_extract=True, use_pretrained=True, model_name=\"squeezenet\",\n",
    "                    classifier=None):\n",
    "    if model_name == \"squeezenet\":\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = classifier\n",
    "    elif model_name == \"resnet\":\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.fc = classifier\n",
    "    else:\n",
    "        raise ValueError(model_name)\n",
    "\n",
    "    model_ft.num_classes = num_classes\n",
    "    return model_ft\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../fitzpatrick17k.csv\")\n",
    "print(df['fitzpatrick'].value_counts())\n",
    "print(\"Rows: {}\".format(df.shape[0]))\n",
    "df[\"low\"] = df['label'].astype('category').cat.codes\n",
    "df[\"mid\"] = df['nine_partition_label'].astype('category').cat.codes\n",
    "df[\"high\"] = df['three_partition_label'].astype('category').cat.codes\n",
    "df[\"hasher\"] = df[\"md5hash\"]\n",
    "df[df[\"fitzpatrick\"].isin([1, 2])][\"low\"].value_counts()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In the custom_load() function, make sure to specify the path to the images\n",
    "print(\"\\nPlease specify number of epochs and 'dev' mode or not... e.g. python train.py 10 full \\n\")\n",
    "reps = 20\n",
    "for n_iter in range(1):\n",
    "    for model_type in [\"squeezenet\"]:#[\"resnet\"]:\n",
    "        for holdout_set in [\"random-holdout\"]:#[\"random-holdout\"]:#[\"spur-light\", \"spur-dark\"]:\n",
    "            n_epochs = 25\n",
    "            eval_every = 5\n",
    "            name_config = f\"{model_type}_{n_epochs}_{holdout_set}_low_{n_iter}\"\n",
    "\n",
    "            dev_mode = \"full\"\n",
    "            \n",
    "            print(\"CUDA is available: {} \\n\".format(torch.cuda.is_available()))\n",
    "            print(\"Starting... \\n\")\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            if dev_mode == \"dev\":\n",
    "                df = pd.read_csv(\"../../fitzpatrick17k.csv\").sample(1000)\n",
    "            else:\n",
    "                df = pd.read_csv(\"../../fitzpatrick17k.csv\")\n",
    "            print(df['fitzpatrick'].value_counts())\n",
    "            print(\"Rows: {}\".format(df.shape[0]))\n",
    "            df[\"low\"] = df['label'].astype('category').cat.codes\n",
    "            df[\"mid\"] = df['nine_partition_label'].astype('category').cat.codes\n",
    "            df[\"high\"] = df['three_partition_label'].astype('category').cat.codes\n",
    "            df[\"hasher\"] = df[\"md5hash\"]\n",
    "            \n",
    "            \n",
    "            if holdout_set == \"expert_select\":\n",
    "                df2 = df\n",
    "                train = df2[df2.qc.isnull()]\n",
    "                test = df2[df2.qc==\"1 Diagnostic\"]\n",
    "            elif holdout_set == \"random-holdout\":\n",
    "                train, test, y_train, y_test = train_test_split(\n",
    "                                                    df,\n",
    "                                                    df.low,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=n_iter,\n",
    "                                                    stratify=df.low)\n",
    "            elif holdout_set == \"a12\":\n",
    "                train = df[(df.fitzpatrick==1)|(df.fitzpatrick==2)]\n",
    "                test = df[(df.fitzpatrick!=1)&(df.fitzpatrick!=2)]\n",
    "                combo = set(train.label.unique()) & set(test.label.unique())\n",
    "                #print(combo)\n",
    "                train = train[train.label.isin(combo)].reset_index()\n",
    "                test = test[test.label.isin(combo)].reset_index()\n",
    "                train[\"low\"] = train['label'].astype('category').cat.codes\n",
    "                test[\"low\"] = test['label'].astype('category').cat.codes\n",
    "            elif holdout_set == \"a56\":\n",
    "                train = df[(df.fitzpatrick==5)|(df.fitzpatrick==6)]\n",
    "                test = df[(df.fitzpatrick!=5)&(df.fitzpatrick!=6)]\n",
    "                combo = set(train.label.unique()) & set(test.label.unique())\n",
    "                train = train[train.label.isin(combo)].reset_index()\n",
    "                test = test[test.label.isin(combo)].reset_index()\n",
    "                train[\"low\"] = train['label'].astype('category').cat.codes\n",
    "                test[\"low\"] = test['label'].astype('category').cat.codes    \n",
    "            train_path = \"temp_train.csv\"\n",
    "            test_path = \"temp_test.csv\"\n",
    "            train.to_csv(train_path, index=False)\n",
    "            test.to_csv(test_path, index=False)\n",
    "            print(\"Training Shape: {}, Test Shape: {} \\n\".format(\n",
    "            train.shape,\n",
    "            test.shape)\n",
    "            )\n",
    "            \n",
    "            indexer, label = 0, \"low\"\n",
    "            weights = np.array(max(train[label].value_counts())/train[label].value_counts().sort_index())\n",
    "            label_codes = sorted(list(train[label].unique()))\n",
    "            print(f\"LABEL CODES: {label_codes}\")\n",
    "            print(len(label_codes))\n",
    "            dataloaders, dataset_sizes = custom_load(\n",
    "                64,\n",
    "                20,\n",
    "                \"{}\".format(train_path),\n",
    "                \"{}\".format(test_path))\n",
    "            \n",
    "            if model_type == \"resnet\":\n",
    "                model_ft = models.resnet18(pretrained=True)\n",
    "                for param in model_ft.parameters():\n",
    "                    param.requires_grad = False\n",
    "                model_ft.fc = nn.Sequential(\n",
    "                        nn.Linear(512, 256), \n",
    "                        nn.ReLU(), \n",
    "                        nn.Dropout(0.4),\n",
    "                        nn.Linear(256, len(label_codes)),                   \n",
    "                        nn.LogSoftmax(dim=1))\n",
    "            \n",
    "            elif model_type == \"squeezenet\":\n",
    "                model_ft = models.squeezenet1_0(pretrained=True)\n",
    "                for param in model_ft.parameters():\n",
    "                    param.requires_grad = False\n",
    "                \n",
    "                #final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
    "                \n",
    "                model_ft.classifier[1] = nn.Sequential(\n",
    "                        nn.Conv2d(512, 256, kernel_size=1), \n",
    "                        nn.ReLU(), \n",
    "                        nn.Dropout(0.4),\n",
    "                        nn.Conv2d(256, len(label_codes), kernel_size=1))\n",
    "            \n",
    "            model_bottom, model_top = get_model_parts(model_ft, \"squeezenet\")\n",
    "                \n",
    "            total_params = sum(p.numel() for p in model_ft.parameters())\n",
    "            print('{} total parameters'.format(total_params))\n",
    "            total_trainable_params = sum(\n",
    "                p.numel() for p in model_ft.parameters() if p.requires_grad)\n",
    "            print('{} total trainable parameters'.format(total_trainable_params))\n",
    "            model_ft = model_ft.to(device)\n",
    "            model_ft = nn.DataParallel(model_ft)\n",
    "            class_weights = torch.FloatTensor(weights).cuda()\n",
    "            criterion = nn.NLLLoss()\n",
    "            optimizer_ft = optim.Adam(model_ft.parameters())\n",
    "            exp_lr_scheduler = lr_scheduler.StepLR(\n",
    "                optimizer_ft,\n",
    "                step_size=7,\n",
    "                gamma=0.1)\n",
    "            print(\"\\nTraining classifier for {}........ \\n\".format(label))        \n",
    "\n",
    "            model_ft, training_results = train_model(\n",
    "                label,\n",
    "                dataloaders, device,\n",
    "                dataset_sizes, model_ft,\n",
    "                criterion, optimizer_ft,\n",
    "                exp_lr_scheduler, n_epochs, eval_every=eval_every)\n",
    "            print(\"Training Complete\")\n",
    "            torch.save(model_ft.state_dict(), os.path.join(OUT_DIR, f\"model-path_{name_config}.pth\"))\n",
    "            training_results.to_csv(os.path.join(OUT_DIR, f\"training_{name_config}.csv\"))\n",
    "            \n",
    "            \n",
    "            model = model_ft.eval()\n",
    "            loader = dataloaders[\"val\"]\n",
    "            prediction_list = []\n",
    "            fitzpatrick_list = []\n",
    "            hasher_list = []\n",
    "            labels_list = []\n",
    "            p_list = []\n",
    "            topk_p = []\n",
    "            topk_n = []\n",
    "            d1 = []\n",
    "            d2 = []\n",
    "            d3 = []\n",
    "            p1 = []\n",
    "            p2 = []\n",
    "            p3 = []\n",
    "            with torch.no_grad():\n",
    "                running_corrects = 0\n",
    "                running_all = 0\n",
    "                for batch in tqdm(loader):\n",
    "                    inputs = batch[\"image\"].to(device)\n",
    "                    classes = batch[label].to(device)\n",
    "                    fitzpatrick = batch[\"fitzpatrick\"]\n",
    "                    hasher = batch[\"hasher\"]\n",
    "                    outputs = model(inputs.float())\n",
    "                    probability = outputs\n",
    "                    ppp, preds = torch.topk(probability, 1)\n",
    "                    if label == \"low\":\n",
    "                        _, preds5 = torch.topk(probability, 3)\n",
    "                        topk_p.append(np.exp(_.cpu()).tolist())\n",
    "                        topk_n.append(preds5.cpu().tolist())\n",
    "                    running_corrects += torch.sum(preds.view(-1) == classes.data)\n",
    "                    running_all += preds.shape[0]\n",
    "                    p_list.append(ppp.cpu().tolist())\n",
    "                    prediction_list.append(preds.cpu().tolist())\n",
    "                    labels_list.append(classes.tolist())\n",
    "                    fitzpatrick_list.append(fitzpatrick.tolist())\n",
    "                    hasher_list.append(hasher)\n",
    "                acc = float(running_corrects)/float(running_all)\n",
    "            if label == \"low\":\n",
    "                for j in topk_n:\n",
    "                    for i in j:\n",
    "                        d1.append(i[0])\n",
    "                        d2.append(i[1])\n",
    "                        d3.append(i[2])\n",
    "                for j in topk_p:\n",
    "                    for i in j:\n",
    "                        p1.append(i[0])\n",
    "                        p2.append(i[1])\n",
    "                        p3.append(i[2])\n",
    "                df_x=pd.DataFrame({\n",
    "                                    \"hasher\": flatten(hasher_list),\n",
    "                                    \"label\": flatten(labels_list),\n",
    "                                    \"fitzpatrick\": flatten(fitzpatrick_list),\n",
    "                                    \"prediction_probability\": flatten(p_list),\n",
    "                                    \"prediction\": flatten(prediction_list),\n",
    "                                    \"d1\": d1,\n",
    "                                    \"d2\": d2,\n",
    "                                    \"d3\": d3,\n",
    "                                    \"p1\": p1,\n",
    "                                    \"p2\": p2,\n",
    "                                    \"p3\": p3})\n",
    "            else:\n",
    "                df_x=pd.DataFrame({\n",
    "                                    \"hasher\": flatten(hasher_list),\n",
    "                                    \"label\": flatten(labels_list),\n",
    "                                    \"fitzpatrick\": flatten(fitzpatrick_list),\n",
    "                                    \"prediction_probability\": flatten(p_list),\n",
    "                                    \"prediction\": flatten(prediction_list)})\n",
    "            \n",
    "            df_x.to_csv(os.path.join(OUT_DIR, f\"results_{name_config}.csv\"),\n",
    "                            index=False)\n",
    "            print(\"\\n Accuracy over {}: {} \\n\".format(holdout_set, acc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
