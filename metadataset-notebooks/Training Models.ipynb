{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from data_utils import prepare_drift_dataset\n",
    "from model_utils import initialize_model\n",
    "from training_utils import train_model\n",
    "import os\n",
    "\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_name = \"resnet\"\n",
    "input_size = 224\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "n_train = 150\n",
    "n_drift = 50\n",
    "SEED = 4\n",
    "num_classes = 5\n",
    "feature_extract = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_root = '/path/dataset/metadataset/MetaDataset/subsets'\n",
    "experiment_root = f'/path/outputs/conceptualexplanations/metadataset/{model_name}'\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target_classes = [\"dog\", \"cat\", \"bird\", \"bear\", \"elephant\"]\n",
    "out_dists = {}\n",
    "out_dists[\"dog\"] = [\"dog(water)\", \"cat(water)\"]\n",
    "out_dists[\"cat\"] = [\"dog(water)\", \"cat(water)\"]\n",
    "out_dists[\"bird\"] = [\"bear(rock)\", \"bird(rock)\"]\n",
    "out_dists[\"bear\"] = [\"bear(rock)\", \"bird(rock)\"]\n",
    "out_dists[\"elephant\"] = [\"bear(grass)\", \"elephant(water)\"]\n",
    "c_alternatives = {\"car\": \"water\", \"water\": \"car\",\n",
    "                  \"rock\": \"water\"}\n",
    "class_concepts = {animal: os.listdir(os.path.join(data_root, animal)) for animal in target_classes}\n",
    "experiments = ['dog(chair)',\n",
    "                'cat(cabinet)',\n",
    "               'dog(snow)',\n",
    "               'dog(car)',\n",
    "               'dog(horse)',\n",
    "               'bird(water)',\n",
    "               'dog(water)',\n",
    "               'dog(fence)',\n",
    "               'elephant(building)',\n",
    "               'cat(keyboard)',\n",
    "               'dog(sand)',\n",
    "               'cat(computer)',\n",
    "               'dog(bed)',\n",
    "               'cat(bed)',\n",
    "               'cat(book)',\n",
    "               'dog(grass)',\n",
    "               'cat(mirror)',\n",
    "               'bird(sand)',\n",
    "               'bear(chair)',\n",
    "               'cat(grass)']"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "valid_experiments = {}\n",
    "for animal, concept_list in class_concepts.items():\n",
    "    for concept in concept_list:\n",
    "        if concept not in experiments:\n",
    "            continue\n",
    "        concept_path = os.path.join(data_root, animal, concept)\n",
    "        im_count = len(os.listdir(concept_path))\n",
    "        if im_count < 50:\n",
    "            print(f\"#Images for {concept}: {im_count}. Skipping for now.\")\n",
    "            continue\n",
    "        in_distributions = [concept if a==animal else a for a in target_classes]\n",
    "        out_distributions = []\n",
    "        valid_experiments[concept] = {}\n",
    "        valid_experiments[concept][\"animal\"] = animal\n",
    "        valid_experiments[concept][\"path\"] = concept_path\n",
    "        valid_experiments[concept][\"im_count\"] = im_count\n",
    "        valid_experiments[concept][\"n_train\"] = min(im_count, n_train)\n",
    "        valid_experiments[concept][\"in_distributions\"] = [concept if a==animal else a for a in target_classes]\n",
    "        out_dist = out_dists[animal]\n",
    "        if concept in out_dist:\n",
    "            c_name = concept.split(\"(\")[1][:-1]\n",
    "            alt = c_alternatives[c_name]\n",
    "            out_dist = [d.replace(c_name, alt) for d in out_dist]\n",
    "        valid_experiments[concept][\"out_distributions\"] = out_dist"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Datasets and Train models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_folders = {}\n",
    "\n",
    "for concept, concept_config in valid_experiments.items():\n",
    "    if os.path.exists(os.path.join(experiment_root, concept)):\n",
    "        print(f\"{concept} model is already trained!\")\n",
    "        continue\n",
    "    print(concept, concept_config)\n",
    "    # Data augmentation and normalization for training\n",
    "    # Just normalization for validation\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "         folders = prepare_drift_dataset(data_root, experiment_root, \n",
    "                                        concept_config[\"in_distributions\"], \n",
    "                                        concept_config[\"out_distributions\"], seed=SEED,\n",
    "                                        n_train=concept_config[\"n_train\"], n_drift=n_drift)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping this experiment due to the following error: {e}\")\n",
    "        continue\n",
    "    \n",
    "    all_folders[concept] = folders\n",
    "    \n",
    "    # Create training and validation datasets\n",
    "    image_datasets = {x: datasets.ImageFolder(folders[x], data_transforms[x]) for x in ['train', 'val']}\n",
    "    # Create training and validation dataloaders\n",
    "    dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "    print(\"Datasets and loaders are ready.\")\n",
    "    model_ft, input_size = initialize_model(num_classes, feature_extract, use_pretrained=True, model_name=model_name)\n",
    "    model_ft = model_ft.to(device)\n",
    "    \n",
    "    params_to_update = model_ft.parameters()\n",
    "    print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name,param in model_ft.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\",name)\n",
    "    else:\n",
    "        for name,param in model_ft.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                print(\"\\t\",name)\n",
    "    \n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.Adam(params_to_update, lr=0.001)\n",
    "    \n",
    "    # Setup the loss fxn\n",
    "    criterion = nn.CrossEntropyLoss()    \n",
    "    \n",
    "    model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"),\n",
    "                                device=device)\n",
    "    best_val_acc = np.max(hist).item()\n",
    "    hist = [h.item() for h in hist]\n",
    "\n",
    "    result_log = {}\n",
    "    result_log[\"val_acc_hist\"] = hist\n",
    "    result_log[\"best_val_acc\"] = best_val_acc\n",
    "    result_log[\"in_dists\"] = concept_config[\"in_distributions\"]\n",
    "    result_log[\"out_dists\"] = concept_config[\"out_distributions\"]\n",
    "    result_log[\"seed\"] = SEED\n",
    "    with open(os.path.join(folders[\"result\"], \"result_log.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(result_log, f)\n",
    "    \n",
    "    with open(os.path.join(folders[\"result\"], \"concept_config.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(concept_config, f)\n",
    "        \n",
    "    torch.save(model_ft, open(os.path.join(folders[\"result\"], \"confounded-model.pt\"), \"wb\"))\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}