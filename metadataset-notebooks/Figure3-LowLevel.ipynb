{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from model_utils import get_model_parts\n",
    "from argparse import Namespace \n",
    "from concept_utils import ConceptBank\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_root = '/path/metadataset/MetaDataset/subsets'\n",
    "experiment_root = '/path/outputs/conceptualexplanations/metadataset/resnet_mv_scale_50'\n",
    "\n",
    "args = Namespace()\n",
    "args.model_name = \"squeezenet\"\n",
    "args.input_size = 224\n",
    "args.batch_size = 8\n",
    "args.num_epochs = 5\n",
    "args.SEED = 4\n",
    "args.num_classes = 1000\n",
    "args.feature_extract = True\n",
    "args.num_workers = 4\n",
    "args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.bank_path = '/path/conceptual-explanations/banks/concept_squeezenet_170.pkl'\n",
    "mean_pxs = np.array([0.485, 0.456, 0.406])\n",
    "std_pxs = np.array([0.229, 0.224, 0.225])\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(args.input_size),\n",
    "    transforms.CenterCrop(args.input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_pxs, std_pxs)\n",
    "])"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_concepts = pickle.load(open(args.bank_path, 'rb'))\n",
    "concept_bank = ConceptBank(all_concepts, args.device)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set up the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get the model \n",
    "model_ft = models.squeezenet1_0(pretrained=True)\n",
    "# Model to test\n",
    "model_bottom, model_top = get_model_parts(model_ft, args.model_name)\n",
    "model_bottom.eval()\n",
    "model_top.eval()\n",
    "model_bottom = model_bottom.to(args.device)\n",
    "model_top = model_top.to(args.device)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation Methods"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "response = requests.get(\"https://git.io/JJkYN\")\n",
    "class_labels = response.text.split(\"\\n\")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def rgb_to_hsv(rgb):\n",
    "    # Translated from source of colorsys.rgb_to_hsv\n",
    "    # r,g,b should be a numpy arrays with values between 0 and 255\n",
    "    # rgb_to_hsv returns an array of floats between 0.0 and 1.0.\n",
    "    rgb = rgb.astype('float')\n",
    "    hsv = np.zeros_like(rgb)\n",
    "    # in case an RGBA array was passed, just copy the A channel\n",
    "    hsv[..., 3:] = rgb[..., 3:]\n",
    "    r, g, b = rgb[..., 0], rgb[..., 1], rgb[..., 2]\n",
    "    maxc = np.max(rgb[..., :3], axis=-1)\n",
    "    minc = np.min(rgb[..., :3], axis=-1)\n",
    "    hsv[..., 2] = maxc\n",
    "    mask = maxc != minc\n",
    "    hsv[mask, 1] = (maxc - minc)[mask] / maxc[mask]\n",
    "    rc = np.zeros_like(r)\n",
    "    gc = np.zeros_like(g)\n",
    "    bc = np.zeros_like(b)\n",
    "    rc[mask] = (maxc - r)[mask] / (maxc - minc)[mask]\n",
    "    gc[mask] = (maxc - g)[mask] / (maxc - minc)[mask]\n",
    "    bc[mask] = (maxc - b)[mask] / (maxc - minc)[mask]\n",
    "    hsv[..., 0] = np.select(\n",
    "        [r == maxc, g == maxc], [bc - gc, 2.0 + rc - bc], default=4.0 + gc - rc)\n",
    "    hsv[..., 0] = (hsv[..., 0] / 6.0) % 1.0\n",
    "    return hsv\n",
    "\n",
    "def hsv_to_rgb(hsv):\n",
    "    # Translated from source of colorsys.hsv_to_rgb\n",
    "    # h,s should be a numpy arrays with values between 0.0 and 1.0\n",
    "    # v should be a numpy array with values between 0.0 and 255.0\n",
    "    # hsv_to_rgb returns an array of uints between 0 and 255.\n",
    "    rgb = np.empty_like(hsv)\n",
    "    rgb[..., 3:] = hsv[..., 3:]\n",
    "    h, s, v = hsv[..., 0], hsv[..., 1], hsv[..., 2]\n",
    "    i = (h * 6.0).astype('uint8')\n",
    "    f = (h * 6.0) - i\n",
    "    p = v * (1.0 - s)\n",
    "    q = v * (1.0 - s * f)\n",
    "    t = v * (1.0 - s * (1.0 - f))\n",
    "    i = i % 6\n",
    "    conditions = [s == 0.0, i == 1, i == 2, i == 3, i == 4, i == 5]\n",
    "    rgb[..., 0] = np.select(conditions, [v, q, p, p, t, v], default=v)\n",
    "    rgb[..., 1] = np.select(conditions, [v, v, v, q, p, p], default=t)\n",
    "    rgb[..., 2] = np.select(conditions, [v, p, t, v, v, q], default=p)\n",
    "    return rgb.astype('uint8')\n",
    "\n",
    "def shift_hue(arr,hout):\n",
    "    hsv=rgb_to_hsv(arr)\n",
    "    hsv[...,0]=hout\n",
    "    rgb=hsv_to_rgb(hsv)\n",
    "    return rgb\n",
    "\n",
    "def colorize(image, hue):\n",
    "    arr = np.array(image)\n",
    "    arr_ = shift_hue(arr, hue)\n",
    "    image_ = Image.fromarray(arr_)\n",
    "    return image_\n",
    "\n",
    "def get_concept_scores_mv_valid(tensor, labels, concept_bank, model_bottom, model_top, \n",
    "                                alpha=1e-4, beta=1e-4, n_steps=100,\n",
    "                                lr=1e-1, momentum=0.9, enforce_validity=True):\n",
    "    \n",
    "    max_margins = concept_bank.margin_info.max\n",
    "    min_margins = concept_bank.margin_info.min\n",
    "    concept_norms = concept_bank.norms\n",
    "    concept_intercepts = concept_bank.intercepts\n",
    "    concepts = concept_bank.bank\n",
    "    concept_names = concept_bank.concept_names.copy()\n",
    "    device = tensor.device\n",
    "    embedding = model_bottom(tensor)\n",
    "    embedding = embedding.detach()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    W = nn.Parameter(torch.zeros(1, concepts.shape[0], device=device), requires_grad=True)\n",
    "    \n",
    "    # Normalize the concept vectors\n",
    "    normalized_C = max_margins * concepts / concept_norms\n",
    "    \n",
    "    # Compute the current distance of the sample to decision boundaries of SVMs\n",
    "    margins = (torch.matmul(concepts, embedding.T) + concept_intercepts) / concept_norms\n",
    "    \n",
    "    # Computing constraints for the concepts scores\n",
    "    W_clamp_max = (max_margins*concept_norms - concept_intercepts - torch.matmul(concepts, embedding.T))\n",
    "    W_clamp_min = (min_margins*concept_norms - concept_intercepts - torch.matmul(concepts, embedding.T))\n",
    "    W_clamp_max = (W_clamp_max / (max_margins * concept_norms)).detach().T\n",
    "    W_clamp_min = (W_clamp_min / (max_margins * concept_norms)).detach().T\n",
    "    \n",
    "    if enforce_validity:\n",
    "        W_clamp_max[(margins > 0).T] = 0.\n",
    "        W_clamp_min[(margins < 0).T] = 0.\n",
    "    \n",
    "    optimizer = optim.SGD([W], lr=lr, momentum=momentum)\n",
    "    history = []\n",
    "    es = n_steps\n",
    "    for i in range(n_steps):\n",
    "        optimizer.zero_grad()\n",
    "        new_embedding = embedding + torch.matmul(W, normalized_C)\n",
    "        new_out = model_top(new_embedding)\n",
    "        l1_loss = torch.norm(W, dim=1, p=1)\n",
    "        l2_loss = torch.norm(W, dim=1, p=2)\n",
    "        ce_loss = criterion(new_out, labels)\n",
    "        loss = ce_loss + l1_loss*alpha + l2_loss*beta\n",
    "\n",
    "        #print(loss.item(), ce_loss.item(), l1_loss.item(), l2_loss.item())\n",
    "        loss.backward()\n",
    "        pred = new_out.argmax(dim=1).detach().item()\n",
    "        history.append(f\"{pred}, {ce_loss.item()}, {l1_loss.item()}, {l2_loss.item()}, {W[0, 0]}, {W.grad[0, 0]}, {W.sum()}\")\n",
    "        optimizer.step()\n",
    "        if enforce_validity:\n",
    "            W_projected = torch.where(W < W_clamp_min, W_clamp_min, W)\n",
    "            W_projected = torch.where(W > W_clamp_max, W_clamp_max, W_projected)\n",
    "            W.data = W_projected\n",
    "            W.grad.zero_()\n",
    "    \n",
    "    final_emb = embedding + torch.matmul(W, normalized_C)\n",
    "    W = W[0].detach().cpu().numpy().tolist()\n",
    "    \n",
    "    concept_scores = dict()\n",
    "    for i, n in enumerate(concept_names): \n",
    "        concept_scores[n] = W[i] \n",
    "    concept_names = sorted(concept_names, key=concept_scores.get, reverse=True)  \n",
    "    \n",
    "    new_out, orig_out = model_top(final_emb), model_top(embedding)\n",
    "    if (new_out.argmax(dim=1) == labels):\n",
    "        success = True\n",
    "    else:\n",
    "        success = False\n",
    "    return success, concept_scores, concept_names, np.array(W)\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img = PIL.Image.open(\"green_apple.jpeg\").convert(\"RGB\")\n",
    "img_ = PIL.Image.open(\"green_apple.jpeg\").convert(\"L\").convert(\"RGB\")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preds = []\n",
    "ces = []\n",
    "images = []\n",
    "\n",
    "alphas = np.concatenate([np.linspace(0., 0.3, 16), np.linspace(0.3, 0.6, 4)])\n",
    "#alphas = np.linspace(0.1, 0.5, 15)\n",
    "\n",
    "labels = torch.tensor(class_labels.index(\"Granny Smith\")).long().view(1).to(args.device) \n",
    "\n",
    "for alpha in tqdm(alphas):\n",
    "    average_img = PIL.Image.fromarray(np.array(alpha*np.array(img) + (1-alpha)*np.array(img_), dtype=np.uint8))\n",
    "    images.append(average_img)\n",
    "    tensor = data_transforms(average_img).unsqueeze(0).to(args.device)\n",
    "    success, concept_scores, concept_scores_list, W_old = get_concept_scores_mv_valid(tensor, labels, \n",
    "                                                                                      concept_bank, \n",
    "                                                                                      model_bottom, model_top,\n",
    "                                                                                      alpha=1e-2, beta=1e-1, lr=1e-2)\n",
    "\n",
    "    pred = model_top(model_bottom(tensor)).detach().cpu().numpy()[0, class_labels.index(\"Granny Smith\")]\n",
    "    preds.append(pred)\n",
    "    ces.append(concept_scores['greenness'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.imshow(images[0])\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"./paper_figures/fig4_gray0.pdf\")\n",
    "plt.savefig(\"./paper_figures/fig4_gray0.png\")\n",
    "plt.close()\n",
    "plt.imshow(images[len(images)//2])\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"./paper_figures/fig4_gray_half.png\")\n",
    "plt.savefig(\"./paper_figures/fig4_gray_half.pdf\")\n",
    "plt.close()\n",
    "plt.imshow(images[-1])\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"./paper_figures/fig4_gray_1.pdf\")\n",
    "plt.savefig(\"./paper_figures/fig4_gray_1.png\")\n",
    "plt.close()\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=[7, 5])\n",
    "plt.plot(np.linspace(0,1,len(alphas)), np.array(ces)[::-1], marker='o', color='green', label='\\'Greenness\\' CCE')\n",
    "plt.plot(np.linspace(0,1,len(alphas)), preds[::-1], marker='o', color='black', label='\\'Granny Smith\\' prob predicted ')\n",
    "\n",
    "plt.yticks(fontname='Arial', fontsize=18)\n",
    "plt.xticks(fontname='Arial', fontsize=16)\n",
    "plt.xlabel('Degree of perturbation', fontname='Arial', fontsize=18)\n",
    "plt.legend(prop={'family':'Arial', 'size':16}, loc=\"upper right\")\n",
    "plt.savefig(\"./paper_figures/fig4_low_level_img.png\")\n",
    "plt.savefig(\"./paper_figures/fig4_low_level_img.pdf\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 50 Images"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "from google_images_download import google_images_download   #importing the library\n",
    "response = google_images_download.googleimagesdownload()   #class instantiation\n",
    "arguments = {\"keywords\":\"granny smith apple\",\"limit\":25,\"print_urls\":True, \"size\": \"medium\",\n",
    "            \"metadata\":True}   #creating list of arguments\n",
    "paths = response.download(arguments)   #passing the arguments to the function\n",
    "\n",
    "print(paths)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "paths = [os.path.join(\"./downloads/granny smith apple\", f) for f in os.listdir(\"./downloads/granny smith apple/\")]"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ces_scores = []\n",
    "#img_paths = paths[0]['granny smith apple']\n",
    "img_paths = paths\n",
    "labels = torch.tensor(class_labels.index(\"Granny Smith\")).long().view(1).to(args.device) \n",
    "for path in tqdm(img_paths[2:]):\n",
    "    img = PIL.Image.open(path).convert(\"RGB\")\n",
    "    img_ = PIL.Image.open(path).convert(\"L\").convert(\"RGB\")\n",
    "    #plt.imshow(img_)\n",
    "    ces_img = []\n",
    "    alphas = np.concatenate([np.linspace(0., 0.3, 16), np.linspace(0.3, 0.6, 4)])\n",
    "    for alpha in alphas:\n",
    "        average_img = PIL.Image.fromarray(np.array(alpha*np.array(img) + (1-alpha)*np.array(img_), dtype=np.uint8))\n",
    "        tensor = data_transforms(average_img).unsqueeze(0).to(args.device)\n",
    "        success, concept_scores, concept_scores_list, W_old = get_concept_scores_mv_valid(tensor, labels, \n",
    "                                                                                          concept_bank, \n",
    "                                                                                          model_bottom, model_top,\n",
    "                                                                                          alpha=0., beta=1e-2, lr=1.,\n",
    "                                                                                          enforce_validity=True)\n",
    "        \n",
    "        pred = model_top(model_bottom(tensor)).detach().cpu().numpy()[0, class_labels.index(\"Granny Smith\")]\n",
    "        ces_img.append(concept_scores['greenness'])\n",
    "    ces_scores.append(ces_img)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=[7, 5])\n",
    "\n",
    "ces_normalized = []\n",
    "for k in range(len(ces_scores)):\n",
    "    img_ces = np.array(ces_scores[k])\n",
    "    normalized_ces = img_ces\n",
    "    ces_normalized.append(normalized_ces)\n",
    "    plt.plot(np.linspace(0, 1, len(alphas)), np.flip(normalized_ces), color='gray', lw=1)\n",
    "\n",
    "    \n",
    "plt.plot(np.linspace(0, 1, len(alphas)), np.mean(np.flip(np.array(ces_normalized), axis=1), axis=0), color='green', lw=4, marker='o')\n",
    "\n",
    "\n",
    "plt.yticks(fontname='Arial', fontsize=18)\n",
    "plt.xticks(fontname='Arial', fontsize=16)\n",
    "plt.ylabel('Greenness CCE', fontname='Arial', fontsize=18)\n",
    "plt.xlabel('Degree of perturbation', fontname='Arial', fontsize=18)\n",
    "plt.savefig(\"./paper_figures/fig4_low_level.png\")\n",
    "plt.savefig(\"./paper_figures/fig4_low_level.pdf\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}